---
title       : Vision over Transit Incidents and Claims
subtitle    : Data Driven Approaches to Reducing Insurance Costs to TransLink
author      : Merve Sahin, Brayden Tang, Simardeep Kaur, Xugang Zhong
job         : UBC MDS Capstone
framework   : io2012        # {io2012, html5slides, shower, dzslides, ...}
highlighter : prettify      # {highlight.js, prettify, highlight}
hitheme     : tomorrow      # 
widgets     : [bootstrap]            # {mathjax, quiz, bootstrap}
mode        : selfcontained # {standalone, draft}
knit        : slidify::knit2slides
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

--- {.build}
## Business Question - high costs

> * Insurance premium is one of the largest spendings in TransLink's budget
> * In the past five years, claim costs have increased by about **122.5%**
> * Therefore, we have been asked to find:
  - .fragment potential strong predictors of claim severity/frequency that TransLink can leverage to help reduce costs

---
## Research Questions

- What are the main predictors of the frequency and severity of bus accidents?
       - Driver characteristics (probation period, experience)
       - Claim types and other accident descriptions
       - Bus model/model year
       - Bus routes
       - Acceleration/decceleration
       - Weather
       - Time
       - Geographic location
       - ...
    
- Within specific categorical features (such as claim type codes), are there specific clusters or groupings that are particularly noteworthy for having worse or better claims experience?  

---
## High Level Data Descriptions

- Speed Performance
- Incident Operators (truncation)
- Collisions (Preventable and Non-Preventable)
- Claims

--- {.build}
## Data Product

> * A reproducible, **interactive** report that allows the reader to:
    - .fragment visualize relationships between claim frequency/severity and specific variables interactively
    - .fragment query a predictive model, again interactively
  
<sketch of report here?>

> * A fully reproducible data pipeline
    - .fragment user-friendly way to run the entire data analysis front to back using simple Make commands
    - .fragment stored on a Docker container
    - .fragment detailed documentation describing how to run the analysis and the code

-- 

---
## Methodology

- Join the Collision and Incident Operators datasets with respect to some id (if given) and then split the whole dataset into test and train datasets
- Exploratory Data Analysis on the resulting training set (visualizations to assess potential predictors of interest such as density plots, boxplots, barcharts, etc.)

*** pnotes

- We (hopefully) expect in the coming days the ability to JOIN the datasets together (operator incidents with claims and bus speeds) so that we can properly answer the specific predictive questions asked. This is pretty crucial.
- If we lose the ability to join, we will likely be limited in relationships we can explore since we will have no ability to relate accidents with multiple factors of interest (like speed, location, route, etc.)

---
## Methodology (cont)

- If complete data is provided, a regression (linear regression, tree based learners, or anything that provide variable importances) on incident rate/hour worked based off driver characteristics, time of day, etc.
- A Bayesian regression model to address the problem of truncation in the Incident Operators dataset or a Zero-Truncated Model (if complete data is not provided)
- Cluster Analysis for analysis of specific categorical features like claim type code, claim description (Markov Chains, LDA, DBSCAN)

*** pnotes

- Complete data: as in, no truncation. In other words, we have all of the drivers, **including those who have 0 accidents.**
- If we are forced to work only with those who are observed to have an incident, we can consider truncated counting models (Pr[X = x | X > 0]) either in frequentist or a Bayesian framework, since we are only sampling part of the observed distribution. This assumes a world with no possibility of 0 accidents, that is, in order for an operator to appear as an observation, they must have first gotten into an accident.
- This has the advantage that we do not explicitly model Pr[X = 0] using data in which this case does not exist, simply throw it away.
- The Bayesian method can also be used to assign some subjective probability of Pr[X = 0] in light of no data.
- All of this is moot if we can obtain data for all operators, regardless if they have had accidents or not.
- the cluster analysis is related to the specific question of whether we can find particular coverage codes and/or specific words in accident descriptions that are correlated with particularly bad accident experience

--- 

## Rough Timeline (May 4th - June 10th)

- Milestone I (May 4th - May 25th, 2020):
  - finalize the datasets that we are working with, along with train/test splits
  - plan to complete all of the analysis described above in the methodology by the end of the month
  - this includes model building and answering the specific predictive hypotheses proposed by TransLink
  
- Milestone II (May 26th - June 10th, 2020):
  - start and finish the interactive report
  - check over the data pipeline with a Make file, create Docker container

--- 

## Rough Timeline (June 11th - June 19th)

- Milestone III (June 10th - June 19th, 2020):
  - review report
  - prepare final presentation to TransLink

- Milestone IV (June 20th - June 23rd, 2020):
  - improve interactive report based off feedback from TransLink
  - complete the final report